{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fc1e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\smuel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib qt\n",
    "warnings.filterwarnings(\"ignore\")  # supress warnings from matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c4eaa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set variables\n",
    "use_seed = False\n",
    "threshold = 2  # possible values are: 1, 2, 5, 10\n",
    "# set the number of epochs you want to train the network (default = 300)\n",
    "epochs = 100  # 300\n",
    "save_fig = True  # set True to save the plots\n",
    "\n",
    "global use_trainable_out\n",
    "use_trainable_out = False\n",
    "global use_trainable_tc\n",
    "use_trainable_tc = False\n",
    "global use_dropout\n",
    "use_dropout = False\n",
    "global batch_size\n",
    "batch_size = 128  # 128\n",
    "global lr\n",
    "lr = 0.0015\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8c451f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to safe plots later\n",
    "if save_fig:\n",
    "    path = '../plots'\n",
    "    isExist = os.path.exists(path)\n",
    "\n",
    "    if not isExist:\n",
    "        os.makedirs(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ff6b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single GPU detected. Setting up the simulation there.\n"
     ]
    }
   ],
   "source": [
    "# check for available GPU and distribute work\n",
    "if torch.cuda.device_count() > 1:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    gpu_sel = 1\n",
    "    gpu_av = [torch.cuda.is_available()\n",
    "              for ii in range(torch.cuda.device_count())]\n",
    "    print(\"Detected {} GPUs. The load will be shared.\".format(\n",
    "        torch.cuda.device_count()))\n",
    "    for gpu in range(len(gpu_av)):\n",
    "        if True in gpu_av:\n",
    "            if gpu_av[gpu_sel]:\n",
    "                device = torch.device(\"cuda:\"+str(gpu))\n",
    "                torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "                print(\"Selected GPUs: {}\" .format(\"cuda:\"+str(gpu)))\n",
    "            else:\n",
    "                device = torch.device(\"cuda:\"+str(gpu_av.index(True)))\n",
    "                # torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            print(\"No GPU detected. Running on CPU.\")\n",
    "else:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(\"Single GPU detected. Setting up the simulation there.\")\n",
    "        device = torch.device(\"cuda:0\")\n",
    "        # torch.cuda.set_per_process_memory_fraction(0.5, device=device)\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"No GPU detected. Running on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2291f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle data randomly\n"
     ]
    }
   ],
   "source": [
    "# use fixed seed for reproducable results\n",
    "if use_seed:\n",
    "    seed = 42  # \"Answer to the Ultimate Question of Life, the Universe, and Everything\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    print(\"Seed set to {}\".format(seed))\n",
    "else:\n",
    "    print(\"Shuffle data randomly\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a7d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03bcccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = ['Space', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K',\n",
    "           'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03ad5dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract(params, file_name, taxels=None, letter_written=letters):\n",
    "\n",
    "    max_time = int(51*25)  # ms\n",
    "    time_bin_size = int(params['time_bin_size'])  # ms\n",
    "    global time\n",
    "    time = range(0, max_time, time_bin_size)\n",
    "\n",
    "    global time_step\n",
    "    time_step = time_bin_size*0.001\n",
    "    data_steps = len(time)\n",
    "\n",
    "    data_dict = pd.read_pickle(file_name)\n",
    "\n",
    "    # Extract data\n",
    "    data = []\n",
    "    labels = []\n",
    "    bins = 1000  # ms conversion\n",
    "    nchan = len(data_dict['events'][1])  # number of channels per sensor\n",
    "    # loop over all trials\n",
    "    for i, sample in enumerate(data_dict['events']):\n",
    "        events_array = np.zeros(\n",
    "            [nchan, round((max_time/time_bin_size)+0.5), 2])\n",
    "        # loop over sensors (taxel)\n",
    "        for taxel in range(len(sample)):\n",
    "            # loop over On and Off channels\n",
    "            for event_type in range(len(sample[taxel])):\n",
    "                if sample[taxel][event_type]:\n",
    "                    indx = bins*(np.array(sample[taxel][event_type]))\n",
    "                    indx = np.array((indx/time_bin_size).round(), dtype=int)\n",
    "                    events_array[taxel, indx, event_type] = 1\n",
    "        if taxels != None:\n",
    "            events_array = np.reshape(np.transpose(events_array, (1, 0, 2))[\n",
    "                                      :, taxels, :], (events_array.shape[1], -1))\n",
    "            selected_chans = 2*len(taxels)\n",
    "        else:\n",
    "            events_array = np.reshape(np.transpose(\n",
    "                events_array, (1, 0, 2)), (events_array.shape[1], -1))\n",
    "            selected_chans = 2*nchan\n",
    "        data.append(events_array)\n",
    "        labels.append(letter_written.index(data_dict['letter'][i]))\n",
    "\n",
    "    # return data,labels\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "    # print(labels)\n",
    "    data = torch.as_tensor(data, dtype=dtype) # torch.tensor() always copies the data\n",
    "    labels = torch.as_tensor(labels, dtype=torch.long)\n",
    "\n",
    "    # create 70/20/10 train/test/validation split\n",
    "    # first create 70/30 train/(test + validation)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        data, labels, test_size=0.30, shuffle=True, stratify=labels)\n",
    "    # split test and validation 2/1\n",
    "    x_test, x_validation, y_test, y_validation = train_test_split(\n",
    "        x_test, y_test, test_size=0.33, shuffle=True, stratify=y_test)\n",
    "\n",
    "    ds_train = TensorDataset(x_train, y_train)\n",
    "    ds_test = TensorDataset(x_test, y_test)\n",
    "    ds_validation = TensorDataset(x_validation, y_validation)\n",
    "\n",
    "    return ds_train, ds_test, ds_validation, labels, selected_chans, data_steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebfd634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_snn(inputs, layers):\n",
    "\n",
    "    if use_trainable_out and use_trainable_tc:\n",
    "        w1, w2, v1, alpha1, beta1, alpha2, beta2, out_scale, out_offset = layers\n",
    "    elif use_trainable_tc:\n",
    "        w1, w2, v1, alpha1, beta1, alpha2, beta2 = layers\n",
    "    elif use_trainable_out:\n",
    "        w1, w2, v1, out_scale, out_offset = layers\n",
    "    else:\n",
    "        w1, w2, v1 = layers\n",
    "    if use_dropout:\n",
    "        dropout = nn.Dropout(p = 0.25) # using dropout on n % of spikes\n",
    "    if use_trainable_tc:\n",
    "        alpha1, beta1 = torch.abs(alpha1), torch.abs(beta1)\n",
    "        alpha2, beta2 = torch.abs(alpha2), torch.abs(beta2)\n",
    "\n",
    "    bs = inputs.shape[0]\n",
    "    \n",
    "    h1 = torch.einsum(\n",
    "        \"abc,cd->abd\", (inputs.tile((nb_input_copies,)), w1))\n",
    "    if use_dropout:\n",
    "        h1 = dropout(h1)\n",
    "    if use_trainable_tc:\n",
    "        spk_rec, mem_rec = recurrent_layer.compute_activity_tc(bs, nb_hidden, h1, v1, alpha1, beta1, nb_steps)\n",
    "    else:\n",
    "        spk_rec, mem_rec = recurrent_layer.compute_activity(bs, nb_hidden, h1, v1, nb_steps)\n",
    "    \n",
    "    # Readout layer\n",
    "    h2 = torch.einsum(\"abc,cd->abd\", (spk_rec, w2))\n",
    "    if use_dropout:\n",
    "        h2 = dropout(h2)\n",
    "    if use_trainable_tc:\n",
    "        s_out_rec, out_rec = feedforward_layer.compute_activity_tc(bs, nb_outputs, h2, alpha2, beta2, nb_steps)\n",
    "    else:\n",
    "        s_out_rec, out_rec = feedforward_layer.compute_activity(bs, nb_outputs, h2, nb_steps)\n",
    "\n",
    "    if use_trainable_out:\n",
    "        # trainable output spike scaling\n",
    "        # mean_firing_rate = torch.div(torch.sum(s_out_rec,1), s_out_rec.shape[1]) # mean firing rate\n",
    "        # s_out_rec = mean_firing_rate*layers[5] + layers[6]\n",
    "        s_out_rec = torch.sum(s_out_rec, 1)*out_scale + \\\n",
    "            out_offset  # sum spikes\n",
    "\n",
    "    other_recs = [mem_rec, spk_rec, out_rec]\n",
    "    layers_update = layers\n",
    "\n",
    "    return s_out_rec, other_recs, layers_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c4b7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_layers(file, map_location, requires_grad=True, variable=False):\n",
    "\n",
    "    if variable:\n",
    "        lays = file\n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "    else:\n",
    "        lays = torch.load(file, map_location=map_location)\n",
    "        for ii in lays:\n",
    "            ii.requires_grad = requires_grad\n",
    "    return lays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42d208c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, dataset, lr=0.0015, nb_epochs=300, opt_parameters=None, layers=None, dataset_test=None):\n",
    "\n",
    "    if (opt_parameters != None) & (layers != None):\n",
    "        parameters = opt_parameters  # The paramters we want to optimize\n",
    "        layers = layers\n",
    "    elif (opt_parameters != None) & (layers == None):\n",
    "        parameters = opt_parameters\n",
    "        if use_trainable_out and use_trainable_tc:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, out_scale, out_offset]\n",
    "        elif use_trainable_out:\n",
    "            layers = [w1, w2, v1, out_scale, out_offset]\n",
    "        elif use_trainable_tc:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "        else:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2,]\n",
    "    elif (opt_parameters == None) & (layers != None):\n",
    "        if use_trainable_out and use_trainable_tc:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                        beta2, out_scale, out_offset]\n",
    "        elif use_trainable_out:\n",
    "            layers = [w1, w2, v1, out_scale, out_offset]\n",
    "        elif use_trainable_tc:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "        else:\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "        layers = layers\n",
    "    elif (opt_parameters == None) & (layers == None):\n",
    "        if use_trainable_out and use_trainable_tc:\n",
    "            parameters = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                        beta2, out_scale, out_offset]\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                        beta2, out_scale, out_offset]\n",
    "        elif use_trainable_out:\n",
    "            parameters = [w1, w2, v1, out_scale, out_offset]\n",
    "            layers = [w1, w2, v1, out_scale, out_offset]\n",
    "        elif use_trainable_tc:\n",
    "            parameters = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "        else:\n",
    "            parameters = [w1, w2, v1, alpha1, beta1,\n",
    "                        alpha2, beta2]\n",
    "            layers = [w1, w2, v1, alpha1, beta1,\n",
    "                        alpha2, beta2]\n",
    "\n",
    "    # The log softmax function across output units\n",
    "    log_softmax_fn = nn.LogSoftmax(dim=1)\n",
    "    loss_fn = nn.NLLLoss()  # The negative log likelihood loss function\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=batch_size,\n",
    "                           shuffle=True, num_workers=2)\n",
    "\n",
    "    # The optimization loop\n",
    "    loss_hist = []\n",
    "    accs_hist = [[], []]\n",
    "    for e in range(nb_epochs):\n",
    "        # learning rate decreases over epochs\n",
    "        optimizer = torch.optim.Adamax(parameters, lr=lr, betas=(0.9, 0.995))\n",
    "        # if e > nb_epochs/2:\n",
    "        #     lr = lr * 0.9\n",
    "        local_loss = []\n",
    "        # accs: mean training accuracies for each batch\n",
    "        accs = []\n",
    "        for x_local, y_local in generator:\n",
    "            x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "            spks_out, recs, layers_update = run_snn(x_local, layers)\n",
    "            # [mem_rec, spk_rec, out_rec]\n",
    "            _, spk_rec, _ = recs\n",
    "\n",
    "            # with output spikes\n",
    "            if use_trainable_out:\n",
    "                m = spks_out\n",
    "            else:\n",
    "                m = torch.sum(spks_out, 1)  # sum over time\n",
    "            # cross entropy loss on the active read-out layer\n",
    "            log_p_y = log_softmax_fn(m)\n",
    "\n",
    "            # TODO change to loop!\n",
    "            # Here we can set up our regularizer loss\n",
    "            # reg_loss = params['reg_spikes']*torch.mean(torch.sum(spks1,1)) # L1 loss on spikes per neuron (original)\n",
    "            # L1 loss on total number of spikes (hidden layer 1)\n",
    "            reg_loss = params['reg_spikes'] * torch.mean(torch.sum(spk_rec, 1))\n",
    "            # L1 loss on total number of spikes (output layer)\n",
    "            # reg_loss += params['reg_spikes']*torch.mean(torch.sum(spks_out, 1))\n",
    "            # print(\"L1: \", reg_loss)\n",
    "            # reg_loss += params['reg_neurons']*torch.mean(torch.sum(torch.sum(spks1,dim=0),dim=0)**2) # e.g., L2 loss on total number of spikes (original)\n",
    "            # L2 loss on spikes per neuron (hidden layer 1)\n",
    "            reg_loss += params['reg_neurons'] * \\\n",
    "                torch.mean(torch.sum(torch.sum(spk_rec, dim=0), dim=0)**2)\n",
    "            # L2 loss on spikes per neuron (output layer)\n",
    "            # reg_loss += params['reg_neurons'] * \\\n",
    "            #     torch.mean(torch.sum(torch.sum(spks_out, dim=0), dim=0)**2)\n",
    "            # print(\"L1 + L2: \", reg_loss)\n",
    "\n",
    "            # Here we combine supervised loss and the regularizer\n",
    "            loss_val = loss_fn(log_p_y, y_local) + reg_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss_val.backward()\n",
    "            optimizer.step()\n",
    "            local_loss.append(loss_val.item())\n",
    "\n",
    "            # compare to labels\n",
    "            _, am = torch.max(m, 1)  # argmax over output units\n",
    "            tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "            accs.append(tmp)\n",
    "\n",
    "        mean_loss = np.mean(local_loss)\n",
    "        loss_hist.append(mean_loss)\n",
    "\n",
    "        # mean_accs: mean training accuracy of current epoch (average over all batches)\n",
    "        mean_accs = np.mean(accs)\n",
    "        accs_hist[0].append(mean_accs)\n",
    "\n",
    "        # Calculate test accuracy in each epoch\n",
    "        if dataset_test is not None:\n",
    "            test_acc = compute_classification_accuracy(\n",
    "                params,\n",
    "                dataset_test,\n",
    "                layers=layers_update\n",
    "            )\n",
    "            accs_hist[1].append(test_acc)  # only safe best test\n",
    "\n",
    "        if dataset_test is None:\n",
    "            # save best training\n",
    "            if mean_accs >= np.max(accs_hist[0]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "        else:\n",
    "            # save best test\n",
    "            if np.max(test_acc) >= np.max(accs_hist[1]):\n",
    "                best_acc_layers = []\n",
    "                for ii in layers_update:\n",
    "                    best_acc_layers.append(ii.detach().clone())\n",
    "\n",
    "        plt.figure(\"live plot\")\n",
    "        plt.suptitle(\"Epoch: {}\" .format(e+1))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, len(accs_hist[0])+1),\n",
    "                 100*np.array(accs_hist[0]), color='blue')\n",
    "        plt.plot(range(1, len(accs_hist[1])+1),\n",
    "                 100*np.array(accs_hist[1]), color='orange')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy (%)\")\n",
    "        plt.ylim(0, 105)\n",
    "        plt.legend([\"Training\", \"Test\"], loc='lower right')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(range(1, len(loss_hist)+1), np.array(loss_hist), color='blue')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend([\"Training\"], loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        # to avoid clearing last plot\n",
    "        if (e != epochs-1):\n",
    "            plt.draw()\n",
    "            plt.pause(0.1)\n",
    "            plt.cla()\n",
    "        else:\n",
    "            plt.close(\"live plot\")\n",
    "\n",
    "        print(\"Epoch {}/{} done. Train accuracy: {:.2f}%, Test accuracy: {:.2f}%, Loss: {:.5f}.\".format(\n",
    "            e + 1, nb_epochs, accs_hist[0][-1]*100, accs_hist[1][-1]*100, loss_hist[-1]))\n",
    "\n",
    "    return loss_hist, accs_hist, best_acc_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb74d6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(params, ds_train, ds_test, epochs=epochs):\n",
    "\n",
    "    global nb_input_copies\n",
    "    # Num of spiking neurons used to encode each channel\n",
    "    nb_input_copies = params['nb_input_copies']\n",
    "\n",
    "    # Network parameters\n",
    "    global nb_inputs\n",
    "    nb_inputs = nb_channels*nb_input_copies\n",
    "    global nb_outputs\n",
    "    nb_outputs = len(np.unique(labels))\n",
    "    global nb_hidden\n",
    "    nb_hidden = 450\n",
    "    global nb_steps\n",
    "    nb_steps = data_steps\n",
    "\n",
    "    tau_mem = params['tau_mem']  # ms\n",
    "    tau_syn = tau_mem/params['tau_ratio']\n",
    "    \n",
    "    if not use_trainable_tc:\n",
    "        global alpha\n",
    "        global beta\n",
    "    alpha = float(np.exp(-time_step/tau_syn))\n",
    "    beta = float(np.exp(-time_step/tau_mem))\n",
    "\n",
    "    fwd_weight_scale = params['fwd_weight_scale']\n",
    "    rec_weight_scale = fwd_weight_scale*params['weight_scale_factor']\n",
    "\n",
    "    # Spiking network\n",
    "    layers = []\n",
    "    \n",
    "    # recurrent layer\n",
    "    w1, v1 = recurrent_layer.create_layer(\n",
    "        nb_inputs, nb_hidden, fwd_weight_scale, rec_weight_scale)\n",
    "\n",
    "    # readout layer\n",
    "    w2 = feedforward_layer.create_layer(\n",
    "        nb_hidden, nb_outputs, fwd_weight_scale)\n",
    "    \n",
    "    if use_trainable_tc:\n",
    "        # time constants\n",
    "        alpha1, beta1 = trainable_time_constants.create_time_constants(\n",
    "            nb_hidden, alpha, beta, use_trainable_tc)\n",
    "\n",
    "        alpha2, beta2 = trainable_time_constants.create_time_constants(\n",
    "            nb_outputs, alpha, beta, use_trainable_tc)\n",
    "\n",
    "\n",
    "    layers.append(w1), layers.append(w2), layers.append(v1)\n",
    "    if use_trainable_tc:\n",
    "        layers.append(alpha1), layers.append(beta1), layers.append(alpha2), layers.append(beta2)\n",
    "\n",
    "    if use_trainable_out:\n",
    "        # include trainable output for readout layer (linear: y = out_scale * x + out_offset)\n",
    "        out_scale = torch.empty(\n",
    "            (nb_outputs),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.ones_(out_scale)\n",
    "        layers.append(out_scale)\n",
    "        out_offset = torch.empty(\n",
    "            (nb_outputs),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.zeros_(out_offset)\n",
    "        layers.append(out_offset)\n",
    "\n",
    "    layers_init = []\n",
    "    for ii in layers:\n",
    "        layers_init.append(ii.detach().clone())\n",
    "\n",
    "    if use_trainable_out and use_trainable_tc:\n",
    "        opt_parameters = [w1, w2, v1, alpha1, beta1, alpha2, beta2, out_scale, out_offset]\n",
    "    elif use_trainable_tc:\n",
    "        opt_parameters = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "    elif use_trainable_out:\n",
    "        opt_parameters = [w1, w2, v1, out_scale, out_offset]\n",
    "    else:\n",
    "        opt_parameters = [w1, w2, v1]\n",
    "\n",
    "    # a fixed learning rate is already defined within the train function, that's why here it is omitted\n",
    "    loss_hist, accs_hist, best_layers = train(\n",
    "        params, ds_train, lr=lr, nb_epochs=epochs, opt_parameters=opt_parameters, layers=layers, dataset_test=ds_test)\n",
    "\n",
    "    # best training and test at best training\n",
    "    acc_best_train = np.max(accs_hist[0])  # returns max value\n",
    "    acc_best_train = acc_best_train*100\n",
    "    idx_best_train = np.argmax(accs_hist[0])  # returns index of max value\n",
    "    acc_test_at_best_train = accs_hist[1][idx_best_train]*100\n",
    "\n",
    "    # best test and training at best test\n",
    "    acc_best_test = np.max(accs_hist[1])\n",
    "    acc_best_test = acc_best_test*100\n",
    "    idx_best_test = np.argmax(accs_hist[1])\n",
    "    acc_train_at_best_test = accs_hist[0][idx_best_test]*100\n",
    "\n",
    "    # TODO track time constants!!!\n",
    "    print(\"Final results: \")\n",
    "    print(\"Best training accuracy: {:.2f}% and according test accuracy: {:.2f}% at epoch: {}\".format(\n",
    "        acc_best_train, acc_test_at_best_train, idx_best_train+1))\n",
    "    print(\"Best test accuracy: {:.2f}% and according train accuracy: {:.2f}% at epoch: {}\".format(\n",
    "        acc_best_test, acc_train_at_best_test, idx_best_test+1))\n",
    "    print(\"------------------------------------------------------------------------------------\\n\")\n",
    "    return loss_hist, accs_hist, best_layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f786ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_classification_accuracy(params, dataset, layers=None):\n",
    "    \"\"\" Computes classification accuracy on supplied data in batches. \"\"\"\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            if use_trainable_out and use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                          beta2, out_scale, out_offset]\n",
    "            elif use_trainable_out:\n",
    "                layers = [w1, w2, v1, out_scale, out_offset]\n",
    "            elif use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            else:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            spks_out, _, _ = run_snn(x_local, layers)\n",
    "        else:\n",
    "            spks_out, _, _ = run_snn(x_local, layers)\n",
    "        # with output spikes\n",
    "        if use_trainable_out:\n",
    "            m = spks_out\n",
    "        else:\n",
    "            m = torch.sum(spks_out, 1)  # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "\n",
    "    return np.mean(accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fe0a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(dataset, save, layers=None, labels=letters):\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=2)\n",
    "    accs = []\n",
    "    trues = []\n",
    "    preds = []\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            if use_trainable_out and use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                          beta2, out_scale, out_offset]\n",
    "            elif use_trainable_out:\n",
    "                layers = [w1, w2, v1, out_scale, out_offset]\n",
    "            elif use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            else:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            spks_out, _, _ = run_snn(x_local, layers)\n",
    "        else:\n",
    "            spks_out, _, _ = run_snn(x_local, layers)\n",
    "        # with output spikes\n",
    "        if use_trainable_out:\n",
    "            m = spks_out\n",
    "        else:\n",
    "            m = torch.sum(spks_out, 1)  # sum over time\n",
    "        _, am = torch.max(m, 1)     # argmax over output units\n",
    "        # compare to labels\n",
    "        tmp = np.mean((y_local == am).detach().cpu().numpy())\n",
    "        accs.append(tmp)\n",
    "        trues.extend(y_local.detach().cpu().numpy())\n",
    "        preds.extend(am.detach().cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(trues, preds, normalize='true')\n",
    "    cm_df = pd.DataFrame(cm, index=[ii for ii in labels], columns=[\n",
    "                         jj for jj in labels])\n",
    "    plt.figure(figsize=(12, 9))\n",
    "    sn.heatmap(cm_df,\n",
    "               annot=True,\n",
    "               fmt='.1g',\n",
    "               cbar=False,\n",
    "               square=False,\n",
    "               cmap=\"YlGnBu\")\n",
    "    plt.xlabel('\\nPredicted')\n",
    "    plt.ylabel('True\\n')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        if use_trainable_out:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_output_optimized_thr_\" +\n",
    "                        str(threshold) + \"_cm.png\", dpi=300)\n",
    "        else:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_thr_\" +\n",
    "                        str(threshold) + \"_cm.png\", dpi=300)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bfd799d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NetworkActivity(dataset, save, layers=None):\n",
    "\n",
    "    generator = DataLoader(dataset, batch_size=batch_size,\n",
    "                           shuffle=False, num_workers=2)\n",
    "\n",
    "    for x_local, y_local in generator:\n",
    "        x_local, y_local = x_local.to(device), y_local.to(device)\n",
    "        if layers == None:\n",
    "            if use_trainable_out and use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2,\n",
    "                          beta2, out_scale, out_offset]\n",
    "            elif use_trainable_out:\n",
    "                layers = [w1, w2, v1, out_scale, out_offset]\n",
    "            elif use_trainable_tc:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            else:\n",
    "                layers = [w1, w2, v1, alpha1, beta1, alpha2, beta2]\n",
    "            spks_out, recs, _ = run_snn(x_local, layers)\n",
    "        else:\n",
    "            spks_out, recs, _ = run_snn(x_local, layers)\n",
    "\n",
    "        # [mem_rec, spk_rec, out_rec]\n",
    "        _, spk_rec, _ = recs\n",
    "\n",
    "    nb_plt = 4\n",
    "    gs = GridSpec(1, nb_plt)\n",
    "\n",
    "    # hidden layer\n",
    "    fig = plt.figure(figsize=(7, 3), dpi=150)\n",
    "    plt.title(\"Hidden layer 1\")\n",
    "    for i in range(nb_plt):\n",
    "        plt.subplot(gs[i])\n",
    "        plt.imshow(spk_rec[i].detach().cpu().numpy().T,\n",
    "                   cmap=plt.cm.gray_r, origin=\"lower\")\n",
    "        if i == 0:\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Units\")\n",
    "        sn.despine()\n",
    "        plt.tight_layout()\n",
    "    if save:\n",
    "        if use_trainable_out:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_output\" +\n",
    "                        \"_thr_\" + str(threshold) + \"_rp_layer_1.png\", dpi=300)\n",
    "        else:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_thr_\" +\n",
    "                        str(threshold) + \"_rp_layer_1.png\", dpi=300)\n",
    "\n",
    "    # output layer\n",
    "    fig = plt.figure(figsize=(7, 3), dpi=150)\n",
    "    plt.title(\"Output layer\")\n",
    "    for i in range(nb_plt):\n",
    "        plt.subplot(gs[i])\n",
    "        plt.imshow(spks_out[i].detach().cpu().numpy().T,\n",
    "                   cmap=plt.cm.gray_r, origin=\"lower\")\n",
    "        if i == 0:\n",
    "            plt.xlabel(\"Time\")\n",
    "            plt.ylabel(\"Units\")\n",
    "        sn.despine()\n",
    "        plt.tight_layout()\n",
    "    if save:\n",
    "        if use_trainable_out:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_output\" +\n",
    "                        \"_thr_\" + str(threshold) + \"_rp_output_layer.png\", dpi=300)\n",
    "        else:\n",
    "            plt.savefig(\"../plots/rsnn_1layers_train_tc_thr_\" +\n",
    "                        str(threshold) + \"_rp_output_layer.png\", dpi=300)\n",
    "    else:\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b201b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and parameters\n",
    "file_dir_data = '../old_dataset/'\n",
    "file_type = 'data_braille_letters_th_'\n",
    "file_thr = str(threshold)\n",
    "file_name = file_dir_data + file_type + file_thr + '.pkl'\n",
    "\n",
    "file_dir_params = '../parameters/'\n",
    "param_filename = 'parameters_th' + str(threshold) + '.txt'\n",
    "file_name_parameters = file_dir_params + param_filename\n",
    "params = {}\n",
    "with open(file_name_parameters) as file:\n",
    "    for line in file:\n",
    "        (key, value) = line.split()\n",
    "        if key == 'time_bin_size' or key == 'nb_input_copies':\n",
    "            params[key] = int(value)\n",
    "        else:\n",
    "            params[key] = np.double(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faabd621",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SurrGradSpike(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Here we implement our spiking nonlinearity which also implements \n",
    "    the surrogate gradient. By subclassing torch.autograd.Function, \n",
    "    we will be able to use all of PyTorch's autograd functionality.\n",
    "    Here we use the normalized negative part of a fast sigmoid \n",
    "    as this was done in Zenke & Ganguli (2018).\n",
    "    \"\"\"\n",
    "\n",
    "    scale = params['scale']\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we compute a step function of the input Tensor\n",
    "        and return it. ctx is a context object that we use to stash information which \n",
    "        we need to later backpropagate our error signals. To achieve this we use the \n",
    "        ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor we need to compute the \n",
    "        surrogate gradient of the loss with respect to the input. \n",
    "        Here we use the normalized negative part of a fast sigmoid \n",
    "        as this was done in Zenke & Ganguli (2018).\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad = grad_input/(SurrGradSpike.scale*torch.abs(input)+1.0)**2\n",
    "        return grad\n",
    "\n",
    "\n",
    "spike_fn = SurrGradSpike.apply\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7be3b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class feedforward_layer:\n",
    "    '''\n",
    "    class to initialize and compute spiking feedforward layer\n",
    "    '''\n",
    "    def create_layer(nb_inputs, nb_outputs, scale):\n",
    "        ff_layer = torch.empty((nb_inputs, nb_outputs),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(ff_layer, mean=0.0, std=scale/np.sqrt(nb_inputs))\n",
    "        return ff_layer\n",
    "    \n",
    "    def compute_activity(nb_input, nb_neurons, input_activity, nb_steps):\n",
    "        syn = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        mem = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        out = torch.zeros((nb_input, nb_neurons), device=device, dtype=dtype)\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "\n",
    "        # Compute feedforward layer activity\n",
    "        for t in range(nb_steps):\n",
    "            mthr = mem-1.0\n",
    "            out = spike_fn(mthr)\n",
    "            rst_out = out  # .detach()\n",
    "\n",
    "            new_syn = alpha*syn + input_activity[:,t]\n",
    "            new_mem = (beta*mem + syn)*(1.0-rst_out)\n",
    "\n",
    "            mem_rec.append(mem)\n",
    "            spk_rec.append(out)\n",
    "\n",
    "            mem = new_mem\n",
    "            syn = new_syn\n",
    "\n",
    "        # Now we merge the recorded membrane potentials into a single tensor\n",
    "        mem_rec = torch.stack(mem_rec,dim=1)\n",
    "        spk_rec = torch.stack(spk_rec,dim=1)\n",
    "        return spk_rec, mem_rec\n",
    "    \n",
    "    def compute_activity_tc(nb_input, nb_neurons, input_activity, alpha, beta, nb_steps):\n",
    "        syn = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        mem = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        out = torch.zeros((nb_input, nb_neurons), device=device, dtype=dtype)\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "\n",
    "        # Compute feedforward layer activity\n",
    "        for t in range(nb_steps):\n",
    "            mthr = mem-1.0\n",
    "            out = spike_fn(mthr)\n",
    "            rst_out = out  # .detach()\n",
    "\n",
    "            new_syn = torch.abs(alpha)*syn + input_activity[:,t]\n",
    "            new_mem = (torch.abs(beta)*mem + syn)*(1.0-rst_out)\n",
    "\n",
    "            mem_rec.append(mem)\n",
    "            spk_rec.append(out)\n",
    "\n",
    "            mem = new_mem\n",
    "            syn = new_syn\n",
    "\n",
    "        # Now we merge the recorded membrane potentials into a single tensor\n",
    "        mem_rec = torch.stack(mem_rec,dim=1)\n",
    "        spk_rec = torch.stack(spk_rec,dim=1)\n",
    "        return spk_rec, mem_rec\n",
    "\n",
    "\n",
    "class recurrent_layer:\n",
    "    '''\n",
    "    class to initialize and compute spiking recurrent layer\n",
    "    '''\n",
    "    def create_layer(nb_inputs, nb_outputs, fwd_scale, rec_scale):\n",
    "        ff_layer = torch.empty((nb_inputs, nb_outputs),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(ff_layer, mean=0.0, std=fwd_scale/np.sqrt(nb_inputs))\n",
    "        \n",
    "        rec_layer = torch.empty((nb_outputs, nb_outputs),  device=device, dtype=dtype, requires_grad=True)\n",
    "        torch.nn.init.normal_(rec_layer, mean=0.0, std=rec_scale/np.sqrt(nb_inputs))\n",
    "        return ff_layer,  rec_layer\n",
    "    \n",
    "    def compute_activity(nb_input, nb_neurons, input_activity, layer, nb_steps):\n",
    "        syn = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        mem = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        out = torch.zeros((nb_input, nb_neurons), device=device, dtype=dtype)\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "\n",
    "        # Compute recurrent layer activity\n",
    "        for t in range(nb_steps):\n",
    "            # input activity plus last step output activity\n",
    "            h1 = input_activity[:,t] + torch.einsum(\"ab,bc->ac\", (out, layer))\n",
    "            mthr = mem-1.0\n",
    "            out = spike_fn(mthr)\n",
    "            rst = out  # .detach() # We do not want to backprop through the reset\n",
    "\n",
    "            new_syn = alpha*syn + h1\n",
    "            new_mem = (beta*mem + syn)*(1.0-rst)\n",
    "\n",
    "            mem_rec.append(mem)\n",
    "            spk_rec.append(out)\n",
    "        \n",
    "            mem = new_mem\n",
    "            syn = new_syn\n",
    "\n",
    "        # Now we merge the recorded membrane potentials into a single tensor\n",
    "        mem_rec = torch.stack(mem_rec,dim=1)\n",
    "        spk_rec = torch.stack(spk_rec,dim=1)\n",
    "        return spk_rec, mem_rec\n",
    "    \n",
    "    def compute_activity_tc(nb_input, nb_neurons, input_activity, layer, alpha, beta, nb_steps):\n",
    "        syn = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        mem = torch.zeros((nb_input,nb_neurons), device=device, dtype=dtype)\n",
    "        out = torch.zeros((nb_input, nb_neurons), device=device, dtype=dtype)\n",
    "        mem_rec = []\n",
    "        spk_rec = []\n",
    "\n",
    "        # Compute recurrent layer activity\n",
    "        for t in range(nb_steps):\n",
    "            # input activity plus last step output activity\n",
    "            h1 = input_activity[:,t] + torch.einsum(\"ab,bc->ac\", (out, layer))\n",
    "            mthr = mem-1.0\n",
    "            out = spike_fn(mthr)\n",
    "            rst = out  # .detach() # We do not want to backprop through the reset\n",
    "\n",
    "            new_syn = torch.abs(alpha)*syn + h1\n",
    "            new_mem = (torch.abs(beta)*mem + syn)*(1.0-rst)\n",
    "\n",
    "            mem_rec.append(mem)\n",
    "            spk_rec.append(out)\n",
    "        \n",
    "            mem = new_mem\n",
    "            syn = new_syn\n",
    "\n",
    "        # Now we merge the recorded membrane potentials into a single tensor\n",
    "        mem_rec = torch.stack(mem_rec,dim=1)\n",
    "        spk_rec = torch.stack(spk_rec,dim=1)\n",
    "        return spk_rec, mem_rec\n",
    "\n",
    "\n",
    "class trainable_time_constants:\n",
    "    def create_time_constants(nb_neurons, alpha_mean, beta_mean, trainable):\n",
    "        alpha = torch.empty((nb_neurons),  device=device,\n",
    "                             dtype=dtype, requires_grad=trainable)\n",
    "        torch.nn.init.normal_(\n",
    "            alpha, mean=alpha_mean, std=alpha_mean/10)\n",
    "        \n",
    "        beta = torch.empty((nb_neurons),  device=device,\n",
    "                            dtype=dtype, requires_grad=trainable)\n",
    "        torch.nn.init.normal_(\n",
    "            beta, mean=beta_mean, std=beta_mean/10)\n",
    "        return alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8fff8c",
   "metadata": {},
   "source": [
    "### Train and test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd9869ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data 3780\n",
      "Number of testing data 1085\n",
      "Number of validation data 535\n",
      "Number of outputs 27\n",
      "Number of timesteps 425\n",
      "Input duration 1.275000s\n",
      "---------------------------\n",
      "\n",
      "Epoch 1/100 done. Train accuracy: 5.45%, Test accuracy: 10.02%, Loss: 23.22600.\n",
      "Epoch 2/100 done. Train accuracy: 10.24%, Test accuracy: 11.94%, Loss: 12.33299.\n",
      "Epoch 3/100 done. Train accuracy: 11.93%, Test accuracy: 15.22%, Loss: 4.36083.\n",
      "Epoch 4/100 done. Train accuracy: 19.72%, Test accuracy: 30.89%, Loss: 3.04355.\n",
      "Epoch 5/100 done. Train accuracy: 26.23%, Test accuracy: 32.29%, Loss: 2.86582.\n",
      "Epoch 6/100 done. Train accuracy: 34.29%, Test accuracy: 40.89%, Loss: 2.68516.\n",
      "Epoch 7/100 done. Train accuracy: 33.28%, Test accuracy: 41.85%, Loss: 2.56467.\n",
      "Epoch 8/100 done. Train accuracy: 40.90%, Test accuracy: 43.10%, Loss: 2.30034.\n",
      "Epoch 9/100 done. Train accuracy: 42.74%, Test accuracy: 49.22%, Loss: 2.29422.\n",
      "Epoch 10/100 done. Train accuracy: 47.05%, Test accuracy: 53.33%, Loss: 2.11694.\n",
      "Epoch 11/100 done. Train accuracy: 46.62%, Test accuracy: 49.54%, Loss: 2.12720.\n",
      "Epoch 12/100 done. Train accuracy: 50.59%, Test accuracy: 55.46%, Loss: 1.91531.\n",
      "Epoch 13/100 done. Train accuracy: 53.26%, Test accuracy: 57.93%, Loss: 1.75602.\n",
      "Epoch 14/100 done. Train accuracy: 56.28%, Test accuracy: 59.58%, Loss: 1.66931.\n",
      "Epoch 15/100 done. Train accuracy: 62.38%, Test accuracy: 60.26%, Loss: 1.37840.\n",
      "Epoch 16/100 done. Train accuracy: 62.24%, Test accuracy: 62.74%, Loss: 1.43392.\n",
      "Epoch 17/100 done. Train accuracy: 65.14%, Test accuracy: 64.28%, Loss: 1.23155.\n",
      "Epoch 18/100 done. Train accuracy: 64.68%, Test accuracy: 63.42%, Loss: 1.41489.\n",
      "Epoch 19/100 done. Train accuracy: 68.28%, Test accuracy: 66.80%, Loss: 1.15144.\n",
      "Epoch 20/100 done. Train accuracy: 70.03%, Test accuracy: 68.04%, Loss: 1.07069.\n",
      "Epoch 21/100 done. Train accuracy: 71.39%, Test accuracy: 68.75%, Loss: 1.03922.\n",
      "Epoch 22/100 done. Train accuracy: 72.87%, Test accuracy: 68.66%, Loss: 0.97193.\n",
      "Epoch 23/100 done. Train accuracy: 72.19%, Test accuracy: 68.27%, Loss: 0.97819.\n",
      "Epoch 24/100 done. Train accuracy: 76.12%, Test accuracy: 69.65%, Loss: 0.81905.\n",
      "Epoch 25/100 done. Train accuracy: 75.17%, Test accuracy: 70.32%, Loss: 0.87560.\n",
      "Epoch 26/100 done. Train accuracy: 77.39%, Test accuracy: 70.97%, Loss: 0.77841.\n",
      "Epoch 27/100 done. Train accuracy: 79.25%, Test accuracy: 72.01%, Loss: 0.73032.\n",
      "Epoch 28/100 done. Train accuracy: 80.09%, Test accuracy: 71.83%, Loss: 0.67963.\n",
      "Epoch 29/100 done. Train accuracy: 80.53%, Test accuracy: 72.23%, Loss: 0.64529.\n",
      "Epoch 30/100 done. Train accuracy: 81.34%, Test accuracy: 73.66%, Loss: 0.62886.\n",
      "Epoch 31/100 done. Train accuracy: 82.09%, Test accuracy: 73.11%, Loss: 0.56943.\n",
      "Epoch 32/100 done. Train accuracy: 83.72%, Test accuracy: 73.36%, Loss: 0.50446.\n",
      "Epoch 33/100 done. Train accuracy: 84.97%, Test accuracy: 71.96%, Loss: 0.51812.\n",
      "Epoch 34/100 done. Train accuracy: 84.90%, Test accuracy: 72.56%, Loss: 0.49279.\n",
      "Epoch 35/100 done. Train accuracy: 86.35%, Test accuracy: 73.98%, Loss: 0.41535.\n",
      "Epoch 36/100 done. Train accuracy: 85.38%, Test accuracy: 74.16%, Loss: 0.46046.\n",
      "Epoch 37/100 done. Train accuracy: 87.01%, Test accuracy: 75.94%, Loss: 0.43126.\n",
      "Epoch 38/100 done. Train accuracy: 86.92%, Test accuracy: 74.44%, Loss: 0.42721.\n",
      "Epoch 39/100 done. Train accuracy: 88.86%, Test accuracy: 74.69%, Loss: 0.37501.\n",
      "Epoch 40/100 done. Train accuracy: 88.12%, Test accuracy: 75.89%, Loss: 0.37445.\n",
      "Epoch 41/100 done. Train accuracy: 88.96%, Test accuracy: 74.25%, Loss: 0.35447.\n",
      "Epoch 42/100 done. Train accuracy: 90.02%, Test accuracy: 75.58%, Loss: 0.31729.\n",
      "Epoch 43/100 done. Train accuracy: 89.00%, Test accuracy: 75.21%, Loss: 0.36793.\n",
      "Epoch 44/100 done. Train accuracy: 90.52%, Test accuracy: 75.40%, Loss: 0.30118.\n",
      "Epoch 45/100 done. Train accuracy: 89.42%, Test accuracy: 75.74%, Loss: 0.35477.\n",
      "Epoch 46/100 done. Train accuracy: 90.71%, Test accuracy: 75.57%, Loss: 0.29008.\n",
      "Epoch 47/100 done. Train accuracy: 92.67%, Test accuracy: 75.21%, Loss: 0.22862.\n",
      "Epoch 48/100 done. Train accuracy: 92.17%, Test accuracy: 75.81%, Loss: 0.23283.\n",
      "Epoch 49/100 done. Train accuracy: 92.80%, Test accuracy: 76.98%, Loss: 0.23047.\n",
      "Epoch 50/100 done. Train accuracy: 92.70%, Test accuracy: 78.69%, Loss: 0.22251.\n",
      "Epoch 51/100 done. Train accuracy: 92.85%, Test accuracy: 75.02%, Loss: 0.22494.\n",
      "Epoch 52/100 done. Train accuracy: 94.44%, Test accuracy: 77.01%, Loss: 0.17546.\n",
      "Epoch 53/100 done. Train accuracy: 92.62%, Test accuracy: 77.67%, Loss: 0.21892.\n",
      "Epoch 54/100 done. Train accuracy: 93.68%, Test accuracy: 75.90%, Loss: 0.19055.\n",
      "Epoch 55/100 done. Train accuracy: 94.25%, Test accuracy: 76.17%, Loss: 0.19012.\n",
      "Epoch 56/100 done. Train accuracy: 94.86%, Test accuracy: 76.43%, Loss: 0.15434.\n",
      "Epoch 57/100 done. Train accuracy: 94.30%, Test accuracy: 77.83%, Loss: 0.17799.\n",
      "Epoch 58/100 done. Train accuracy: 94.87%, Test accuracy: 74.59%, Loss: 0.16565.\n",
      "Epoch 59/100 done. Train accuracy: 94.90%, Test accuracy: 75.92%, Loss: 0.15397.\n",
      "Epoch 60/100 done. Train accuracy: 93.31%, Test accuracy: 75.74%, Loss: 0.23304.\n",
      "Epoch 61/100 done. Train accuracy: 95.42%, Test accuracy: 75.90%, Loss: 0.14633.\n",
      "Epoch 62/100 done. Train accuracy: 95.73%, Test accuracy: 76.32%, Loss: 0.13364.\n",
      "Epoch 63/100 done. Train accuracy: 96.22%, Test accuracy: 76.76%, Loss: 0.11305.\n",
      "Epoch 64/100 done. Train accuracy: 95.86%, Test accuracy: 75.02%, Loss: 0.12505.\n",
      "Epoch 65/100 done. Train accuracy: 95.97%, Test accuracy: 76.76%, Loss: 0.12226.\n",
      "Epoch 66/100 done. Train accuracy: 95.65%, Test accuracy: 75.62%, Loss: 0.15561.\n",
      "Epoch 67/100 done. Train accuracy: 96.54%, Test accuracy: 76.06%, Loss: 0.11280.\n",
      "Epoch 68/100 done. Train accuracy: 95.72%, Test accuracy: 76.26%, Loss: 0.14156.\n",
      "Epoch 69/100 done. Train accuracy: 96.31%, Test accuracy: 77.19%, Loss: 0.12185.\n",
      "Epoch 70/100 done. Train accuracy: 96.48%, Test accuracy: 77.15%, Loss: 0.11962.\n",
      "Epoch 71/100 done. Train accuracy: 96.46%, Test accuracy: 77.66%, Loss: 0.11338.\n",
      "Epoch 72/100 done. Train accuracy: 96.51%, Test accuracy: 78.10%, Loss: 0.10675.\n",
      "Epoch 73/100 done. Train accuracy: 96.67%, Test accuracy: 78.18%, Loss: 0.10825.\n",
      "Epoch 74/100 done. Train accuracy: 96.99%, Test accuracy: 75.46%, Loss: 0.09878.\n",
      "Epoch 75/100 done. Train accuracy: 96.88%, Test accuracy: 76.17%, Loss: 0.10620.\n",
      "Epoch 76/100 done. Train accuracy: 97.68%, Test accuracy: 77.55%, Loss: 0.08963.\n",
      "Epoch 77/100 done. Train accuracy: 96.49%, Test accuracy: 77.59%, Loss: 0.11225.\n",
      "Epoch 78/100 done. Train accuracy: 97.32%, Test accuracy: 77.73%, Loss: 0.08844.\n",
      "Epoch 79/100 done. Train accuracy: 97.14%, Test accuracy: 76.59%, Loss: 0.09990.\n",
      "Epoch 80/100 done. Train accuracy: 97.22%, Test accuracy: 76.61%, Loss: 0.09529.\n",
      "Epoch 81/100 done. Train accuracy: 97.14%, Test accuracy: 77.02%, Loss: 0.09666.\n",
      "Epoch 82/100 done. Train accuracy: 97.01%, Test accuracy: 78.14%, Loss: 0.09469.\n",
      "Epoch 83/100 done. Train accuracy: 97.33%, Test accuracy: 78.63%, Loss: 0.08913.\n",
      "Epoch 84/100 done. Train accuracy: 95.34%, Test accuracy: 77.48%, Loss: 0.17787.\n",
      "Epoch 85/100 done. Train accuracy: 96.83%, Test accuracy: 79.06%, Loss: 0.09631.\n",
      "Epoch 86/100 done. Train accuracy: 97.55%, Test accuracy: 76.87%, Loss: 0.07260.\n",
      "Epoch 87/100 done. Train accuracy: 97.24%, Test accuracy: 79.06%, Loss: 0.09066.\n",
      "Epoch 88/100 done. Train accuracy: 97.11%, Test accuracy: 78.25%, Loss: 0.10257.\n",
      "Epoch 89/100 done. Train accuracy: 97.94%, Test accuracy: 78.54%, Loss: 0.07363.\n",
      "Epoch 90/100 done. Train accuracy: 97.24%, Test accuracy: 77.48%, Loss: 0.09769.\n",
      "Epoch 91/100 done. Train accuracy: 96.77%, Test accuracy: 78.29%, Loss: 0.10021.\n",
      "Epoch 92/100 done. Train accuracy: 98.46%, Test accuracy: 79.59%, Loss: 0.05521.\n",
      "Epoch 93/100 done. Train accuracy: 98.36%, Test accuracy: 78.81%, Loss: 0.07151.\n",
      "Epoch 94/100 done. Train accuracy: 97.76%, Test accuracy: 77.24%, Loss: 0.06669.\n",
      "Epoch 95/100 done. Train accuracy: 97.37%, Test accuracy: 77.75%, Loss: 0.08544.\n",
      "Epoch 96/100 done. Train accuracy: 97.17%, Test accuracy: 77.59%, Loss: 0.10388.\n",
      "Epoch 97/100 done. Train accuracy: 97.40%, Test accuracy: 76.93%, Loss: 0.08067.\n",
      "Epoch 98/100 done. Train accuracy: 97.66%, Test accuracy: 78.98%, Loss: 0.07558.\n",
      "Epoch 99/100 done. Train accuracy: 97.81%, Test accuracy: 78.19%, Loss: 0.07068.\n",
      "Epoch 100/100 done. Train accuracy: 98.23%, Test accuracy: 79.15%, Loss: 0.05994.\n",
      "Final results: \n",
      "Best training accuracy: 98.46% and according test accuracy: 79.59% at epoch: 92\n",
      "Best test accuracy: 79.59% and according train accuracy: 98.46% at epoch: 92\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "*************************\n",
      "* Validation:  79.33423913043478\n",
      "*************************\n",
      "Epoch 1/100 done. Train accuracy: 7.32%, Test accuracy: 11.61%, Loss: 23.44668.\n",
      "Epoch 2/100 done. Train accuracy: 12.90%, Test accuracy: 20.00%, Loss: 21.53949.\n",
      "Epoch 3/100 done. Train accuracy: 18.05%, Test accuracy: 27.51%, Loss: 9.51395.\n",
      "Epoch 4/100 done. Train accuracy: 28.35%, Test accuracy: 33.97%, Loss: 5.40561.\n",
      "Epoch 5/100 done. Train accuracy: 37.18%, Test accuracy: 48.45%, Loss: 3.36394.\n",
      "Epoch 6/100 done. Train accuracy: 45.13%, Test accuracy: 50.09%, Loss: 2.71471.\n",
      "Epoch 7/100 done. Train accuracy: 49.79%, Test accuracy: 53.33%, Loss: 2.44120.\n",
      "Epoch 8/100 done. Train accuracy: 52.70%, Test accuracy: 57.03%, Loss: 2.12722.\n",
      "Epoch 9/100 done. Train accuracy: 57.91%, Test accuracy: 61.87%, Loss: 1.78740.\n",
      "Epoch 10/100 done. Train accuracy: 60.72%, Test accuracy: 60.42%, Loss: 1.63679.\n",
      "Epoch 11/100 done. Train accuracy: 62.69%, Test accuracy: 64.68%, Loss: 1.53289.\n",
      "Epoch 12/100 done. Train accuracy: 64.33%, Test accuracy: 65.64%, Loss: 1.38876.\n",
      "Epoch 13/100 done. Train accuracy: 66.43%, Test accuracy: 65.26%, Loss: 1.42553.\n",
      "Epoch 14/100 done. Train accuracy: 68.96%, Test accuracy: 69.22%, Loss: 1.16271.\n",
      "Epoch 15/100 done. Train accuracy: 69.20%, Test accuracy: 66.37%, Loss: 1.22454.\n",
      "Epoch 16/100 done. Train accuracy: 71.52%, Test accuracy: 68.40%, Loss: 1.08161.\n",
      "Epoch 17/100 done. Train accuracy: 72.25%, Test accuracy: 70.04%, Loss: 1.06227.\n",
      "Epoch 18/100 done. Train accuracy: 75.33%, Test accuracy: 70.03%, Loss: 0.89589.\n",
      "Epoch 19/100 done. Train accuracy: 76.43%, Test accuracy: 67.77%, Loss: 0.89795.\n",
      "Epoch 20/100 done. Train accuracy: 77.35%, Test accuracy: 69.72%, Loss: 0.82741.\n",
      "Epoch 21/100 done. Train accuracy: 78.00%, Test accuracy: 68.99%, Loss: 0.75941.\n",
      "Epoch 22/100 done. Train accuracy: 79.61%, Test accuracy: 72.41%, Loss: 0.71945.\n",
      "Epoch 23/100 done. Train accuracy: 79.10%, Test accuracy: 71.58%, Loss: 0.75567.\n",
      "Epoch 24/100 done. Train accuracy: 79.18%, Test accuracy: 72.50%, Loss: 0.72904.\n",
      "Epoch 25/100 done. Train accuracy: 81.25%, Test accuracy: 71.82%, Loss: 0.63122.\n",
      "Epoch 26/100 done. Train accuracy: 82.07%, Test accuracy: 73.37%, Loss: 0.64743.\n",
      "Epoch 27/100 done. Train accuracy: 82.36%, Test accuracy: 73.38%, Loss: 0.62315.\n",
      "Epoch 28/100 done. Train accuracy: 84.22%, Test accuracy: 72.08%, Loss: 0.53820.\n",
      "Epoch 29/100 done. Train accuracy: 82.24%, Test accuracy: 74.00%, Loss: 0.65434.\n",
      "Epoch 30/100 done. Train accuracy: 84.27%, Test accuracy: 72.50%, Loss: 0.53782.\n",
      "Epoch 31/100 done. Train accuracy: 85.33%, Test accuracy: 74.71%, Loss: 0.49852.\n",
      "Epoch 32/100 done. Train accuracy: 86.63%, Test accuracy: 73.38%, Loss: 0.43641.\n",
      "Epoch 33/100 done. Train accuracy: 88.60%, Test accuracy: 74.77%, Loss: 0.40058.\n",
      "Epoch 34/100 done. Train accuracy: 87.82%, Test accuracy: 75.30%, Loss: 0.40716.\n",
      "Epoch 35/100 done. Train accuracy: 88.87%, Test accuracy: 76.57%, Loss: 0.38530.\n",
      "Epoch 36/100 done. Train accuracy: 88.68%, Test accuracy: 74.60%, Loss: 0.38046.\n",
      "Epoch 37/100 done. Train accuracy: 89.45%, Test accuracy: 76.07%, Loss: 0.32290.\n",
      "Epoch 38/100 done. Train accuracy: 89.47%, Test accuracy: 76.87%, Loss: 0.34015.\n",
      "Epoch 39/100 done. Train accuracy: 90.25%, Test accuracy: 75.72%, Loss: 0.33642.\n",
      "Epoch 40/100 done. Train accuracy: 91.36%, Test accuracy: 76.69%, Loss: 0.29505.\n",
      "Epoch 41/100 done. Train accuracy: 90.50%, Test accuracy: 74.33%, Loss: 0.32893.\n",
      "Epoch 42/100 done. Train accuracy: 90.07%, Test accuracy: 76.17%, Loss: 0.31480.\n",
      "Epoch 43/100 done. Train accuracy: 92.35%, Test accuracy: 74.94%, Loss: 0.27021.\n",
      "Epoch 44/100 done. Train accuracy: 92.49%, Test accuracy: 74.75%, Loss: 0.25172.\n",
      "Epoch 45/100 done. Train accuracy: 93.16%, Test accuracy: 77.49%, Loss: 0.20625.\n",
      "Epoch 46/100 done. Train accuracy: 93.50%, Test accuracy: 76.36%, Loss: 0.20140.\n",
      "Epoch 47/100 done. Train accuracy: 93.40%, Test accuracy: 77.31%, Loss: 0.21457.\n",
      "Epoch 48/100 done. Train accuracy: 93.29%, Test accuracy: 75.63%, Loss: 0.24256.\n",
      "Epoch 49/100 done. Train accuracy: 93.81%, Test accuracy: 75.64%, Loss: 0.19945.\n",
      "Epoch 50/100 done. Train accuracy: 93.57%, Test accuracy: 74.15%, Loss: 0.21008.\n",
      "Epoch 51/100 done. Train accuracy: 94.53%, Test accuracy: 76.36%, Loss: 0.17676.\n",
      "Epoch 52/100 done. Train accuracy: 93.81%, Test accuracy: 76.10%, Loss: 0.18945.\n",
      "Epoch 53/100 done. Train accuracy: 93.50%, Test accuracy: 75.49%, Loss: 0.21993.\n",
      "Epoch 54/100 done. Train accuracy: 94.85%, Test accuracy: 77.21%, Loss: 0.16648.\n",
      "Epoch 55/100 done. Train accuracy: 94.30%, Test accuracy: 76.02%, Loss: 0.20381.\n",
      "Epoch 56/100 done. Train accuracy: 94.74%, Test accuracy: 76.28%, Loss: 0.18406.\n",
      "Epoch 57/100 done. Train accuracy: 94.77%, Test accuracy: 76.69%, Loss: 0.16328.\n",
      "Epoch 58/100 done. Train accuracy: 95.19%, Test accuracy: 77.32%, Loss: 0.16001.\n",
      "Epoch 59/100 done. Train accuracy: 95.70%, Test accuracy: 76.17%, Loss: 0.15522.\n",
      "Epoch 60/100 done. Train accuracy: 95.89%, Test accuracy: 76.85%, Loss: 0.15668.\n",
      "Epoch 61/100 done. Train accuracy: 95.37%, Test accuracy: 78.86%, Loss: 0.15977.\n",
      "Epoch 62/100 done. Train accuracy: 94.62%, Test accuracy: 75.12%, Loss: 0.19011.\n",
      "Epoch 63/100 done. Train accuracy: 95.64%, Test accuracy: 77.49%, Loss: 0.15096.\n",
      "Epoch 64/100 done. Train accuracy: 95.85%, Test accuracy: 77.22%, Loss: 0.14724.\n",
      "Epoch 65/100 done. Train accuracy: 95.21%, Test accuracy: 78.03%, Loss: 0.16726.\n",
      "Epoch 66/100 done. Train accuracy: 95.87%, Test accuracy: 75.96%, Loss: 0.14038.\n",
      "Epoch 67/100 done. Train accuracy: 95.03%, Test accuracy: 77.45%, Loss: 0.16478.\n",
      "Epoch 68/100 done. Train accuracy: 96.17%, Test accuracy: 77.14%, Loss: 0.13241.\n",
      "Epoch 69/100 done. Train accuracy: 95.45%, Test accuracy: 77.58%, Loss: 0.15110.\n",
      "Epoch 70/100 done. Train accuracy: 96.57%, Test accuracy: 79.34%, Loss: 0.11553.\n",
      "Epoch 71/100 done. Train accuracy: 96.28%, Test accuracy: 77.15%, Loss: 0.12112.\n",
      "Epoch 72/100 done. Train accuracy: 96.85%, Test accuracy: 77.22%, Loss: 0.11590.\n",
      "Epoch 73/100 done. Train accuracy: 95.91%, Test accuracy: 77.56%, Loss: 0.14096.\n",
      "Epoch 74/100 done. Train accuracy: 96.72%, Test accuracy: 77.40%, Loss: 0.11048.\n",
      "Epoch 75/100 done. Train accuracy: 95.90%, Test accuracy: 77.14%, Loss: 0.13924.\n",
      "Epoch 76/100 done. Train accuracy: 96.12%, Test accuracy: 77.92%, Loss: 0.14955.\n",
      "Epoch 77/100 done. Train accuracy: 96.91%, Test accuracy: 75.38%, Loss: 0.10619.\n",
      "Epoch 78/100 done. Train accuracy: 97.19%, Test accuracy: 77.55%, Loss: 0.09632.\n",
      "Epoch 79/100 done. Train accuracy: 96.96%, Test accuracy: 77.76%, Loss: 0.11690.\n",
      "Epoch 80/100 done. Train accuracy: 96.04%, Test accuracy: 78.19%, Loss: 0.13573.\n",
      "Epoch 81/100 done. Train accuracy: 97.53%, Test accuracy: 77.91%, Loss: 0.09779.\n",
      "Epoch 82/100 done. Train accuracy: 96.64%, Test accuracy: 76.89%, Loss: 0.11184.\n",
      "Epoch 83/100 done. Train accuracy: 97.42%, Test accuracy: 76.87%, Loss: 0.08627.\n",
      "Epoch 84/100 done. Train accuracy: 97.42%, Test accuracy: 77.75%, Loss: 0.10206.\n",
      "Epoch 85/100 done. Train accuracy: 97.82%, Test accuracy: 78.53%, Loss: 0.07054.\n",
      "Epoch 86/100 done. Train accuracy: 97.58%, Test accuracy: 78.72%, Loss: 0.09528.\n",
      "Epoch 87/100 done. Train accuracy: 97.45%, Test accuracy: 75.91%, Loss: 0.08311.\n",
      "Epoch 88/100 done. Train accuracy: 97.72%, Test accuracy: 78.45%, Loss: 0.07382.\n",
      "Epoch 89/100 done. Train accuracy: 97.01%, Test accuracy: 78.29%, Loss: 0.10860.\n",
      "Epoch 90/100 done. Train accuracy: 96.96%, Test accuracy: 77.41%, Loss: 0.10804.\n",
      "Epoch 91/100 done. Train accuracy: 97.14%, Test accuracy: 78.12%, Loss: 0.11175.\n",
      "Epoch 92/100 done. Train accuracy: 97.35%, Test accuracy: 77.23%, Loss: 0.10375.\n",
      "Epoch 93/100 done. Train accuracy: 97.14%, Test accuracy: 76.89%, Loss: 0.10066.\n",
      "Epoch 94/100 done. Train accuracy: 97.73%, Test accuracy: 78.15%, Loss: 0.08647.\n",
      "Epoch 95/100 done. Train accuracy: 97.74%, Test accuracy: 78.80%, Loss: 0.07625.\n",
      "Epoch 96/100 done. Train accuracy: 98.15%, Test accuracy: 76.35%, Loss: 0.06927.\n",
      "Epoch 97/100 done. Train accuracy: 97.79%, Test accuracy: 77.57%, Loss: 0.08131.\n",
      "Epoch 98/100 done. Train accuracy: 98.15%, Test accuracy: 76.73%, Loss: 0.07995.\n",
      "Epoch 99/100 done. Train accuracy: 98.13%, Test accuracy: 78.20%, Loss: 0.06664.\n",
      "Epoch 100/100 done. Train accuracy: 97.32%, Test accuracy: 76.10%, Loss: 0.09734.\n",
      "Final results: \n",
      "Best training accuracy: 98.15% and according test accuracy: 76.35% at epoch: 96\n",
      "Best test accuracy: 79.34% and according train accuracy: 96.57% at epoch: 70\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "*************************\n",
      "* Validation:  74.60597826086956\n",
      "*************************\n",
      "Epoch 1/100 done. Train accuracy: 8.71%, Test accuracy: 15.55%, Loss: 22.64591.\n",
      "Epoch 2/100 done. Train accuracy: 14.79%, Test accuracy: 25.51%, Loss: 16.46161.\n",
      "Epoch 3/100 done. Train accuracy: 24.14%, Test accuracy: 33.35%, Loss: 6.82138.\n",
      "Epoch 4/100 done. Train accuracy: 35.03%, Test accuracy: 44.95%, Loss: 3.65866.\n",
      "Epoch 5/100 done. Train accuracy: 41.52%, Test accuracy: 50.57%, Loss: 3.28449.\n",
      "Epoch 6/100 done. Train accuracy: 44.60%, Test accuracy: 51.27%, Loss: 3.21386.\n",
      "Epoch 7/100 done. Train accuracy: 50.70%, Test accuracy: 56.10%, Loss: 2.34830.\n",
      "Epoch 8/100 done. Train accuracy: 55.04%, Test accuracy: 58.41%, Loss: 2.03842.\n",
      "Epoch 9/100 done. Train accuracy: 56.15%, Test accuracy: 58.97%, Loss: 2.01882.\n",
      "Epoch 10/100 done. Train accuracy: 60.02%, Test accuracy: 62.10%, Loss: 1.69807.\n",
      "Epoch 11/100 done. Train accuracy: 61.64%, Test accuracy: 63.51%, Loss: 1.45662.\n",
      "Epoch 12/100 done. Train accuracy: 64.47%, Test accuracy: 67.40%, Loss: 1.43782.\n",
      "Epoch 13/100 done. Train accuracy: 66.76%, Test accuracy: 65.05%, Loss: 1.35464.\n",
      "Epoch 14/100 done. Train accuracy: 67.30%, Test accuracy: 67.47%, Loss: 1.18715.\n",
      "Epoch 15/100 done. Train accuracy: 68.28%, Test accuracy: 66.82%, Loss: 1.24716.\n",
      "Epoch 16/100 done. Train accuracy: 71.92%, Test accuracy: 68.58%, Loss: 1.06348.\n",
      "Epoch 17/100 done. Train accuracy: 71.55%, Test accuracy: 69.39%, Loss: 1.09941.\n",
      "Epoch 18/100 done. Train accuracy: 72.94%, Test accuracy: 70.35%, Loss: 0.99314.\n",
      "Epoch 19/100 done. Train accuracy: 74.14%, Test accuracy: 69.54%, Loss: 0.99538.\n",
      "Epoch 20/100 done. Train accuracy: 75.55%, Test accuracy: 70.32%, Loss: 0.87557.\n",
      "Epoch 21/100 done. Train accuracy: 76.21%, Test accuracy: 70.92%, Loss: 0.86944.\n",
      "Epoch 22/100 done. Train accuracy: 75.29%, Test accuracy: 72.02%, Loss: 0.89537.\n",
      "Epoch 23/100 done. Train accuracy: 78.49%, Test accuracy: 72.90%, Loss: 0.80136.\n",
      "Epoch 24/100 done. Train accuracy: 79.08%, Test accuracy: 73.90%, Loss: 0.75625.\n",
      "Epoch 25/100 done. Train accuracy: 80.92%, Test accuracy: 71.98%, Loss: 0.65881.\n",
      "Epoch 26/100 done. Train accuracy: 80.43%, Test accuracy: 74.78%, Loss: 0.71303.\n",
      "Epoch 27/100 done. Train accuracy: 82.32%, Test accuracy: 73.04%, Loss: 0.61194.\n",
      "Epoch 28/100 done. Train accuracy: 82.91%, Test accuracy: 74.33%, Loss: 0.60720.\n",
      "Epoch 29/100 done. Train accuracy: 83.88%, Test accuracy: 71.56%, Loss: 0.55696.\n",
      "Epoch 30/100 done. Train accuracy: 81.94%, Test accuracy: 74.20%, Loss: 0.70364.\n",
      "Epoch 31/100 done. Train accuracy: 84.79%, Test accuracy: 73.47%, Loss: 0.53415.\n",
      "Epoch 32/100 done. Train accuracy: 86.00%, Test accuracy: 74.45%, Loss: 0.49921.\n",
      "Epoch 33/100 done. Train accuracy: 85.36%, Test accuracy: 71.01%, Loss: 0.48280.\n",
      "Epoch 34/100 done. Train accuracy: 87.35%, Test accuracy: 73.21%, Loss: 0.42253.\n",
      "Epoch 35/100 done. Train accuracy: 86.65%, Test accuracy: 74.18%, Loss: 0.49596.\n",
      "Epoch 36/100 done. Train accuracy: 88.39%, Test accuracy: 73.65%, Loss: 0.37433.\n",
      "Epoch 37/100 done. Train accuracy: 88.22%, Test accuracy: 72.75%, Loss: 0.41108.\n",
      "Epoch 38/100 done. Train accuracy: 87.72%, Test accuracy: 74.33%, Loss: 0.42874.\n",
      "Epoch 39/100 done. Train accuracy: 90.52%, Test accuracy: 75.74%, Loss: 0.30644.\n",
      "Epoch 40/100 done. Train accuracy: 89.38%, Test accuracy: 73.64%, Loss: 0.35987.\n",
      "Epoch 41/100 done. Train accuracy: 90.82%, Test accuracy: 76.35%, Loss: 0.31705.\n",
      "Epoch 42/100 done. Train accuracy: 90.17%, Test accuracy: 75.99%, Loss: 0.33981.\n",
      "Epoch 43/100 done. Train accuracy: 90.78%, Test accuracy: 75.38%, Loss: 0.32321.\n",
      "Epoch 44/100 done. Train accuracy: 91.83%, Test accuracy: 74.98%, Loss: 0.27761.\n",
      "Epoch 45/100 done. Train accuracy: 92.10%, Test accuracy: 74.50%, Loss: 0.26328.\n",
      "Epoch 46/100 done. Train accuracy: 92.23%, Test accuracy: 74.77%, Loss: 0.25456.\n",
      "Epoch 47/100 done. Train accuracy: 92.35%, Test accuracy: 76.00%, Loss: 0.27326.\n",
      "Epoch 48/100 done. Train accuracy: 93.03%, Test accuracy: 76.15%, Loss: 0.25293.\n",
      "Epoch 49/100 done. Train accuracy: 92.75%, Test accuracy: 77.45%, Loss: 0.25423.\n",
      "Epoch 50/100 done. Train accuracy: 93.12%, Test accuracy: 75.63%, Loss: 0.24698.\n",
      "Epoch 51/100 done. Train accuracy: 93.29%, Test accuracy: 74.52%, Loss: 0.22046.\n",
      "Epoch 52/100 done. Train accuracy: 93.82%, Test accuracy: 76.08%, Loss: 0.22265.\n",
      "Epoch 53/100 done. Train accuracy: 93.47%, Test accuracy: 75.46%, Loss: 0.24464.\n",
      "Epoch 54/100 done. Train accuracy: 95.16%, Test accuracy: 76.60%, Loss: 0.16440.\n",
      "Epoch 55/100 done. Train accuracy: 94.07%, Test accuracy: 77.15%, Loss: 0.21489.\n",
      "Epoch 56/100 done. Train accuracy: 93.86%, Test accuracy: 75.83%, Loss: 0.18872.\n",
      "Epoch 57/100 done. Train accuracy: 95.11%, Test accuracy: 75.64%, Loss: 0.17007.\n",
      "Epoch 58/100 done. Train accuracy: 95.66%, Test accuracy: 77.03%, Loss: 0.14963.\n",
      "Epoch 59/100 done. Train accuracy: 94.51%, Test accuracy: 77.30%, Loss: 0.19179.\n",
      "Epoch 60/100 done. Train accuracy: 95.13%, Test accuracy: 76.55%, Loss: 0.16825.\n",
      "Epoch 61/100 done. Train accuracy: 94.07%, Test accuracy: 76.34%, Loss: 0.20967.\n",
      "Epoch 62/100 done. Train accuracy: 96.46%, Test accuracy: 76.53%, Loss: 0.13405.\n",
      "Epoch 63/100 done. Train accuracy: 94.39%, Test accuracy: 77.31%, Loss: 0.19950.\n",
      "Epoch 64/100 done. Train accuracy: 95.29%, Test accuracy: 76.64%, Loss: 0.15095.\n",
      "Epoch 65/100 done. Train accuracy: 95.03%, Test accuracy: 75.63%, Loss: 0.15827.\n",
      "Epoch 66/100 done. Train accuracy: 96.46%, Test accuracy: 76.01%, Loss: 0.13513.\n",
      "Epoch 67/100 done. Train accuracy: 95.30%, Test accuracy: 76.27%, Loss: 0.14780.\n",
      "Epoch 68/100 done. Train accuracy: 95.84%, Test accuracy: 75.37%, Loss: 0.15389.\n",
      "Epoch 69/100 done. Train accuracy: 95.19%, Test accuracy: 75.82%, Loss: 0.17581.\n",
      "Epoch 70/100 done. Train accuracy: 95.45%, Test accuracy: 77.02%, Loss: 0.15383.\n",
      "Epoch 71/100 done. Train accuracy: 95.52%, Test accuracy: 75.72%, Loss: 0.15865.\n",
      "Epoch 72/100 done. Train accuracy: 96.33%, Test accuracy: 76.78%, Loss: 0.11863.\n",
      "Epoch 73/100 done. Train accuracy: 96.70%, Test accuracy: 73.42%, Loss: 0.11051.\n",
      "Epoch 74/100 done. Train accuracy: 96.36%, Test accuracy: 76.36%, Loss: 0.12392.\n",
      "Epoch 75/100 done. Train accuracy: 96.38%, Test accuracy: 74.24%, Loss: 0.12865.\n",
      "Epoch 76/100 done. Train accuracy: 97.16%, Test accuracy: 76.96%, Loss: 0.10213.\n",
      "Epoch 77/100 done. Train accuracy: 95.81%, Test accuracy: 76.17%, Loss: 0.16883.\n",
      "Epoch 78/100 done. Train accuracy: 96.96%, Test accuracy: 75.28%, Loss: 0.12000.\n",
      "Epoch 79/100 done. Train accuracy: 96.10%, Test accuracy: 75.90%, Loss: 0.14564.\n",
      "Epoch 80/100 done. Train accuracy: 95.95%, Test accuracy: 74.59%, Loss: 0.11936.\n",
      "Epoch 81/100 done. Train accuracy: 96.49%, Test accuracy: 76.69%, Loss: 0.13803.\n",
      "Epoch 82/100 done. Train accuracy: 95.61%, Test accuracy: 76.51%, Loss: 0.15814.\n",
      "Epoch 83/100 done. Train accuracy: 96.23%, Test accuracy: 77.47%, Loss: 0.13892.\n",
      "Epoch 84/100 done. Train accuracy: 96.83%, Test accuracy: 76.17%, Loss: 0.13002.\n",
      "Epoch 85/100 done. Train accuracy: 97.50%, Test accuracy: 76.17%, Loss: 0.09317.\n",
      "Epoch 86/100 done. Train accuracy: 97.01%, Test accuracy: 76.25%, Loss: 0.11434.\n",
      "Epoch 87/100 done. Train accuracy: 97.40%, Test accuracy: 76.80%, Loss: 0.09286.\n",
      "Epoch 88/100 done. Train accuracy: 96.98%, Test accuracy: 77.21%, Loss: 0.10447.\n",
      "Epoch 89/100 done. Train accuracy: 97.58%, Test accuracy: 75.91%, Loss: 0.08362.\n",
      "Epoch 90/100 done. Train accuracy: 97.03%, Test accuracy: 76.19%, Loss: 0.10817.\n",
      "Epoch 91/100 done. Train accuracy: 97.61%, Test accuracy: 74.84%, Loss: 0.09798.\n",
      "Epoch 92/100 done. Train accuracy: 97.85%, Test accuracy: 77.30%, Loss: 0.08003.\n",
      "Epoch 93/100 done. Train accuracy: 97.63%, Test accuracy: 75.79%, Loss: 0.09521.\n",
      "Epoch 94/100 done. Train accuracy: 97.53%, Test accuracy: 75.99%, Loss: 0.09468.\n",
      "Epoch 95/100 done. Train accuracy: 97.03%, Test accuracy: 76.69%, Loss: 0.11898.\n",
      "Epoch 96/100 done. Train accuracy: 96.98%, Test accuracy: 76.70%, Loss: 0.12737.\n",
      "Epoch 97/100 done. Train accuracy: 97.86%, Test accuracy: 76.50%, Loss: 0.08362.\n",
      "Epoch 98/100 done. Train accuracy: 97.73%, Test accuracy: 76.96%, Loss: 0.08557.\n",
      "Epoch 99/100 done. Train accuracy: 97.66%, Test accuracy: 77.12%, Loss: 0.08522.\n",
      "Epoch 100/100 done. Train accuracy: 97.56%, Test accuracy: 77.48%, Loss: 0.09098.\n",
      "Final results: \n",
      "Best training accuracy: 97.86% and according test accuracy: 76.50% at epoch: 97\n",
      "Best test accuracy: 77.48% and according train accuracy: 97.56% at epoch: 100\n",
      "------------------------------------------------------------------------------------\n",
      "\n",
      "*************************\n",
      "* Validation:  81.07336956521738\n",
      "*************************\n",
      "\n",
      "*************************\n",
      "* Best:  81.07336956521738\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "acc_train_list = []\n",
    "acc_test_list = []\n",
    "max_repetitions = 3\n",
    "# load data\n",
    "ds_train, ds_test, ds_validation, labels, nb_channels, data_steps = load_and_extract(\n",
    "    params, file_name, letter_written=letters)\n",
    "for repetition in range(max_repetitions):\n",
    "    if repetition == 0:\n",
    "        print(\"Number of training data %i\" % len(ds_train))\n",
    "        print(\"Number of testing data %i\" % len(ds_test))\n",
    "        print(\"Number of validation data %i\" % len(ds_validation))\n",
    "        print(\"Number of outputs %i\" % len(np.unique(labels)))\n",
    "        print(\"Number of timesteps %i\" % data_steps)\n",
    "        print(\"Input duration %fs\" % (data_steps*time_step))\n",
    "        print(\"---------------------------\\n\")\n",
    "\n",
    "    # initialize and train network\n",
    "    loss_hist, acc_hist, best_layers = build_and_train(\n",
    "        params, ds_train, ds_test, epochs=epochs)\n",
    "\n",
    "    # get validation results\n",
    "    val_acc = compute_classification_accuracy(\n",
    "                params,\n",
    "                ds_validation,\n",
    "                layers=best_layers\n",
    "            )\n",
    "    print(\"*************************\")\n",
    "    print(\"* Validation: \", val_acc*100)\n",
    "    print(\"*************************\")\n",
    "    # safe overall best layer\n",
    "    if repetition == 0:\n",
    "        very_best_layer = best_layers\n",
    "        best_acc = val_acc\n",
    "    else:\n",
    "        if val_acc > best_acc:\n",
    "            very_best_layer = best_layers\n",
    "            best_acc = val_acc\n",
    "\n",
    "    acc_train_list.append(acc_hist[0])\n",
    "    acc_test_list.append(acc_hist[1])\n",
    "\n",
    "print(\"\\n*************************\")\n",
    "print(\"* Best: \", best_acc*100)\n",
    "print(\"*************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0a5b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the best layer\n",
    "torch.save(very_best_layer, '../model/best_model_th'+str(threshold)+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2701b",
   "metadata": {},
   "source": [
    "### Lets plot the training curve and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4aa7b76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calc mean and std\n",
    "acc_mean_train = np.mean(acc_train_list, axis=0)\n",
    "acc_std_train = np.std(acc_train_list, axis=0)\n",
    "acc_mean_test = np.mean(acc_test_list, axis=0)\n",
    "acc_std_test = np.std(acc_test_list, axis=0)\n",
    "best_trial, best_val_idx = np.where(np.max(acc_test_list) == acc_test_list)\n",
    "best_trial, best_val_idx = best_trial[0], best_val_idx[0]\n",
    "plt.figure()\n",
    "# plot best trial\n",
    "plt.plot(range(1, len(acc_train_list[best_trial])+1), 100*np.array(\n",
    "    acc_train_list[best_trial]), color='blue', linestyle='dashed')\n",
    "plt.plot(range(1, len(acc_test_list[best_trial])+1), 100*np.array(\n",
    "    acc_test_list[best_trial]), color='orangered', linestyle='dashed')\n",
    "# plot mean and std\n",
    "plt.plot(range(1, len(acc_mean_train)+1),\n",
    "         100*np.array(acc_mean_train), color='blue')\n",
    "plt.plot(range(1, len(acc_mean_test)+1), 100 *\n",
    "         np.array(acc_mean_test), color='orangered')\n",
    "plt.fill_between(range(1, len(acc_mean_train)+1), 100*(acc_mean_train+acc_std_train), 100*(\n",
    "    acc_mean_train-acc_std_train), color='cornflowerblue')\n",
    "plt.fill_between(range(1, len(acc_mean_test)+1), 100*(\n",
    "    acc_mean_test+acc_std_test), 100*(acc_mean_test-acc_std_test), color='sandybrown')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.ylim((0, 105))\n",
    "plt.legend([\"Training\", \"Test\"], loc='lower right')\n",
    "if save_fig:\n",
    "    plt.savefig(\"../plots/rsnn_1layers_train_tc_thr_\" +\n",
    "                str(threshold)+\"_acc.png\", dpi=300)\n",
    "else:\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ad5ddfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the confusion matrix\n",
    "ConfusionMatrix(ds_test, layers=very_best_layer, save=save_fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f07656",
   "metadata": {},
   "source": [
    "### Lets create some raster plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c4a533b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the network activity\n",
    "NetworkActivity(ds_test, layers=very_best_layer, save=save_fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f3deb9913135076cb2eddfc1aa03f353553d0efc43f9a141be10678a92397b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
