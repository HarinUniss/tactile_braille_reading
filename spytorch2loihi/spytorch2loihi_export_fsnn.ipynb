{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys, os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from SlayerSNN_src.auto.loihi import denseBlock, convBlock, flattenBlock, poolBlock, Network\n",
    "from SlayerSNN_src.slayerLoihi import spikeLayer as loihi\n",
    "from SlayerSNN_src import utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from IPython.display import HTML\n",
    "\n",
    "from SlayerSNN_src.slayer import spikeLayer as layer\n",
    "from SlayerSNN_src.slayerLoihi import spikeLayer as loihi\n",
    "from SlayerSNN_src.slayerParams import yamlParams as params\n",
    "from SlayerSNN_src.spikeLoss import spikeLoss as loss\n",
    "from SlayerSNN_src.spikeClassifier import spikeClassifier as predict\n",
    "from SlayerSNN_src import spikeFileIO as io\n",
    "from SlayerSNN_src import utils\n",
    "# This will be removed later. Kept for compatibility only\n",
    "from SlayerSNN_src.quantizeParams import quantizeWeights as quantize\n",
    "\n",
    "# Added for debug\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold\n",
    "threshold = 10 # 1, 2, 5, 10\n",
    "run = \"_3\"\n",
    "\n",
    "if threshold == 1:\n",
    "    time_bin_size = 5\n",
    "    nb_input_copies = 2\n",
    "    tau_mem = 0.06\n",
    "    tau_ratio = 10\n",
    "elif threshold == 2:\n",
    "    time_bin_size = 3\n",
    "    nb_input_copies = 8\n",
    "    tau_mem = 0.05\n",
    "    tau_ratio = 10\n",
    "elif threshold == 5:\n",
    "    time_bin_size = 3\n",
    "    nb_input_copies = 4\n",
    "    tau_mem = 0.07\n",
    "    tau_ratio = 10\n",
    "elif threshold == 10:\n",
    "    time_bin_size = 5\n",
    "    nb_input_copies = 2\n",
    "    tau_mem = 0.07\n",
    "    tau_ratio = 10\n",
    "\n",
    "# SpyTorch weights\n",
    "weights_path = \"../weights/SpyTorch_trained_weights_fwd_th\" + str(threshold) + run + \".pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import weights\n",
    "path = weights_path\n",
    "SpyTorch_weights = torch.load(path, map_location=torch.device('cpu'))\n",
    "\n",
    "wgt1_in2hid = SpyTorch_weights[0].detach().numpy()\n",
    "wgt2_hid2out = SpyTorch_weights[1].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 28)\n",
      "[[ 0.09514701 -0.2498929  -0.21983077 ... -0.42935914  0.093075\n",
      "  -0.22871235]\n",
      " [-0.37213632 -0.02317753 -0.06820287 ... -0.22371764 -0.07909746\n",
      "  -0.14899771]\n",
      " [ 0.01635543 -0.2587211  -0.03930256 ...  0.2052648   0.14091352\n",
      "  -0.5218253 ]\n",
      " ...\n",
      " [ 0.21863535  0.16354123  0.47647065 ...  0.12791806  0.12644301\n",
      "  -0.8379483 ]\n",
      " [-0.02220986 -0.42480463 -0.06455369 ... -0.16587071  0.43334395\n",
      "   0.15958126]\n",
      " [ 0.21037246  0.02932504 -0.16226019 ... -0.10203745  0.04537127\n",
      "  -0.2431798 ]]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(SpyTorch_weights[1].detach().numpy()))\n",
    "print(SpyTorch_weights[1].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 450)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(SpyTorch_weights[0].detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgt_scale_calc =  99\n",
      "vDecay_calc =  282\n",
      "iDecay_calc =  2090\n",
      "time_bins =  270\n"
     ]
    }
   ],
   "source": [
    "# Loihi inference parameters approximation\n",
    "wgt_max = np.amax([np.amax(np.abs(wgt1_in2hid)), \n",
    "                   np.amax(np.abs(wgt2_hid2out))])\n",
    "wgt_scale_calc = math.floor(256/wgt_max) # round down\n",
    "tau_syn = tau_mem/tau_ratio\n",
    "alpha = float(np.exp(-(time_bin_size/1000)/tau_syn))\n",
    "beta = float(np.exp(-(time_bin_size/1000)/tau_mem))\n",
    "vDecay_calc = int(4096-4096*beta)\n",
    "iDecay_calc = int(4096-4096*alpha)\n",
    "time_bins = math.ceil(1350/time_bin_size) # round up\n",
    "\n",
    "print(\"wgt_scale_calc = \", wgt_scale_calc)\n",
    "print(\"vDecay_calc = \", vDecay_calc)\n",
    "print(\"iDecay_calc = \", iDecay_calc)\n",
    "print(\"time_bins = \", time_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loihi inference parameters\n",
    "wgt_scale = wgt_scale_calc # scale for all weight\n",
    "vThMant = wgt_scale_calc # vth = vthMant âˆ— 64\n",
    "vDecay = vDecay_calc # tau_mem\n",
    "iDecay = iDecay_calc # tau_syn\n",
    "\n",
    "qtz_step = 2 # weights quantization step\n",
    "rec_scale = 1 # extra scale for recurrent weights\n",
    "refDelay = 1 # refractory delay\n",
    "wgtExp = 0 # 2**(6+wgtExp) * W * spike_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   7.,   68.,  473., 2047., 4895., 6715., 5019., 1914.,  408.,\n",
       "          54.]),\n",
       " array([-255.82756 , -209.13321 , -162.43886 , -115.7445  ,  -69.05015 ,\n",
       "         -22.355797,   24.338556,   71.032906,  117.727264,  164.42162 ,\n",
       "         211.11597 ], dtype=float32),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(SpyTorch_weights[0].detach().numpy().flatten() * wgt_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape weights\n",
    "spy_weights = []\n",
    "\n",
    "spy_weights.append([]) # flatten layer\n",
    "\n",
    "spy_weights.append(np.reshape(\n",
    "                        np.transpose(\n",
    "                            SpyTorch_weights[0].detach().numpy()\n",
    "                        ), \n",
    "                        (450, 24*nb_input_copies, 1, 1, 1)) * wgt_scale)\n",
    "\n",
    "spy_weights.append(np.reshape(\n",
    "                        np.transpose(\n",
    "                            SpyTorch_weights[1].detach().numpy()\n",
    "                        ),\n",
    "                        (28, 450, 1, 1, 1)) * wgt_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the network\n",
    "netDesc = {\n",
    "    'simulation' : {\n",
    "        'Ts': 1,\n",
    "        'tSample': time_bins,\n",
    "    },\n",
    "    'neuron' : {\n",
    "        'type'     : 'LOIHI',\n",
    "        'vThMant'  : vThMant,\n",
    "        'vDecay'   : vDecay,\n",
    "        'iDecay'   : iDecay,\n",
    "        'refDelay' : refDelay,\n",
    "        'wgtExp'   : wgtExp,\n",
    "        'tauRho'   : 1, # useless in inference\n",
    "        'scaleRho' : 1, # useless in inference\n",
    "    },\n",
    "    'layer' : [\n",
    "        {'dim' : \"'\" + str(24*nb_input_copies) + \"x1x1\"}, # Width x Height x Channels\n",
    "        {'dim' : 450, 'delay' : False},\n",
    "        {'dim' : 28, 'delay' : False}\n",
    "    ]\n",
    "}\n",
    "\n",
    "netParams = params(dict=netDesc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recurrentBlock(torch.nn.Module):\n",
    "    def __init__(self, slayer, inFeatures, outFeatures, weightScale, \n",
    "                 preHookFx = lambda x: utils.quantize(x, step=qtz_step), weightNorm=False, \n",
    "                 delay=False, maxDelay=62, countLog=False):\n",
    "        super(recurrentBlock, self).__init__()\n",
    "        self.slayer = slayer\n",
    "        self.weightNorm = weightNorm\n",
    "        if weightNorm is True:\n",
    "            self.weightOp = torch.nn.utils.weight_norm(slayer.dense(\n",
    "                inFeatures, outFeatures, weightScale, preHookFx), name='weight')\n",
    "            self.recWeightOp = torch.nn.utils.weight_norm(slayer.dense(\n",
    "                outFeatures, outFeatures, weightScale, preHookFx), name='recWeight')\n",
    "        else:\n",
    "            self.weightOp = slayer.dense(inFeatures, outFeatures, weightScale, preHookFx)\n",
    "            self.recWeightOp = slayer.dense(outFeatures, outFeatures, weightScale, preHookFx)\n",
    "        self.delayOp  = slayer.delay(outFeatures) if delay is True else None\n",
    "        self.countLog = countLog\n",
    "        self.gradLog = True\n",
    "        self.maxDelay = maxDelay\n",
    "        \n",
    "        self.paramsDict = {\n",
    "            'inFeatures'  : inFeatures,\n",
    "            'outFeatures' : outFeatures,\n",
    "        }\n",
    "    \n",
    "    def forward(self, spike):\n",
    "        spike = self.slayer.spikeLoihi(self.weightOp(spike) + self.recWeightOp(spike))\n",
    "        spike = self.slayer.delayShift(spike, 1)\n",
    "        if self.delayOp is not None:\n",
    "            spike = self.delayOp(spike)\n",
    "        if self.countLog is True:\n",
    "            return spike, torch.sum(spike)\n",
    "        else:\n",
    "            return spike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpyTorch2Loihi(Network):\n",
    "    \n",
    "    def __init__(self, netParams, weights):\n",
    "        super(SpyTorch2Loihi, self).__init__(netParams)\n",
    "        self.preHookFx = lambda x: utils.quantize(x, step=qtz_step)\n",
    "        \n",
    "        # Load the weights trained in spytorch\n",
    "        self.weights = weights\n",
    "        self.recWeights = None\n",
    "    \n",
    "    \n",
    "    def _layerType(self, dim):\n",
    "        if type(dim) is int:\n",
    "            return 'dense'\n",
    "        elif dim.find('c') != -1:\n",
    "            return 'conv'\n",
    "        elif dim.find('avg') != -1:\n",
    "            return 'average'\n",
    "        elif dim.find('a') != -1:\n",
    "            return 'pool'\n",
    "        elif dim.find('x') != -1:\n",
    "            return 'input'\n",
    "        elif dim.find('r') != -1:\n",
    "            return 'recurrent'\n",
    "        else:\n",
    "            raise Exception('Could not parse the layer description. Found {}'.format(dim))\n",
    "        # return [int(i) for i in re.findall(r'\\d+', dim)]\n",
    "\n",
    "    \n",
    "    def _parseLayers(self):\n",
    "        i = 0\n",
    "        blocks = torch.nn.ModuleList()\n",
    "        layerDim = [] # CHW\n",
    "        is1Dconv = False\n",
    "\n",
    "        print('\\nNetwork Architecture:')\n",
    "        # print('=====================')\n",
    "        print(self._tableStr(header=True))\n",
    "\n",
    "        for layer in self.netParams['layer']:\n",
    "            layerType = self._layerType(layer['dim'])\n",
    "            # print(i, layerType)\n",
    "\n",
    "            # If layer has neuron feild, then use the slayer initialized with it and self.netParams['simulation']\n",
    "            if 'neuron' in layer.keys():\n",
    "                print(layerType, 'using individual slayer')\n",
    "                slayer = loihi(layer['neuron'], self.netParams['simulation'])\n",
    "            else:\n",
    "                slayer = self.slayer\n",
    "\n",
    "            if i==0 and self.inputShape is None: \n",
    "                if layerType == 'input':\n",
    "                    self.inputShape = tuple([int(numStr) for numStr in re.findall(r'\\d+', layer['dim'])])\n",
    "                    if len(self.inputShape) == 3:\n",
    "                        layerDim = list(self.inputShape)[::-1]\n",
    "                    elif len(self.inputShape) == 2:\n",
    "                        layerDim = [1, self.inputShape[1], self.inputShape[0]]\n",
    "                    else:\n",
    "                        raise Exception('Could not parse the input dimension. Got {}'.format(self.inputShape))\n",
    "                elif layerType == 'dense':\n",
    "                    self.inputShape = tuple([layer['dim']])\n",
    "                    layerDim = [layer['dim'], 1, 1]\n",
    "                else:\n",
    "                    raise Exception('Input dimension could not be determined! It should be the first entry in the' \n",
    "                                    + \"'layer' feild.\")\n",
    "                # print(self.inputShape)\n",
    "                print(self._tableStr('Input', layerDim[2], layerDim[1], layerDim[0]))\n",
    "                if layerDim[1] == 1:\n",
    "                    is1Dconv = True\n",
    "            else:\n",
    "                # print(i, layer['dim'], self._layerType(layer['dim']))\n",
    "                if layerType == 'conv':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    inChannels  = layerDim[0]\n",
    "                    outChannels = params[0]\n",
    "                    kernelSize  = params[1]\n",
    "                    stride      = layer['stride']   if 'stride'   in layer.keys() else 1\n",
    "                    padding     = layer['padding']  if 'padding'  in layer.keys() else kernelSize//2\n",
    "                    dilation    = layer['dilation'] if 'dilation' in layer.keys() else 1\n",
    "                    groups      = layer['groups']   if 'groups'   in layer.keys() else 1\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    # print(i, inChannels, outChannels, kernelSize, stride, padding, dilation, groups, weightScale)\n",
    "                    \n",
    "                    if is1Dconv is False:\n",
    "                        blocks.append(convBlock(slayer, inChannels, outChannels, kernelSize, stride, padding, \n",
    "                                                dilation, groups, weightScale, self.preHookFx, self.weightNorm, \n",
    "                                                delay, maxDelay, self.countLog))\n",
    "                        layerDim[0] = outChannels\n",
    "                        layerDim[1] = int(np.floor((layerDim[1] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                        layerDim[2] = int(np.floor((layerDim[2] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                    else:\n",
    "                        blocks.append(convBlock(slayer, inChannels, outChannels, [1, kernelSize], [1, stride], [0, padding], \n",
    "                                                [1, dilation], groups, weightScale, self.preHookFx, self.weightNorm, \n",
    "                                                delay, maxDelay, self.countLog))\n",
    "                        layerDim[0] = outChannels\n",
    "                        layerDim[1] = 1\n",
    "                        layerDim[2] = int(np.floor((layerDim[2] + 2*padding - dilation * (kernelSize - 1) - 1)/stride + 1))\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Conv', layerDim[2], layerDim[1], layerDim[0], kernelSize, stride, padding, \n",
    "                          delay, sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'pool':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    # print(params[0])\n",
    "                    \n",
    "                    blocks.append(poolBlock(slayer, params[0], countLog=self.countLog))\n",
    "                    layerDim[1] = int(np.ceil(layerDim[1] / params[0]))\n",
    "                    layerDim[2] = int(np.ceil(layerDim[2] / params[0]))\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Pool', layerDim[2], layerDim[1], layerDim[0], params[0]))\n",
    "                elif layerType == 'dense':\n",
    "                    params = layer['dim']\n",
    "                    # print(params)\n",
    "                    if layerDim[1] != 1 or layerDim[2] != 1: # needs flattening of layers\n",
    "                        blocks.append(flattenBlock(self.countLog ))\n",
    "                        layerDim[0] = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "                        layerDim[1] = layerDim[2] = 1\n",
    "                        self.layerDims.append(layerDim.copy())\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    \n",
    "                    blocks.append(denseBlock(slayer, layerDim[0], params, weightScale, self.preHookFx, \n",
    "                                  self.weightNorm, delay, maxDelay, self.countLog))\n",
    "                    layerDim[0] = params\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Dense', layerDim[2], layerDim[1], layerDim[0], delay=delay, \n",
    "                                        numParams=sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'recurrent':\n",
    "                    #params = layer['dim']\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    # print(params)\n",
    "                    if layerDim[1] != 1 or layerDim[2] != 1: # needs flattening of layers\n",
    "                        blocks.append(flattenBlock(self.countLog ))\n",
    "                        layerDim[0] = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "                        layerDim[1] = layerDim[2] = 1\n",
    "                        self.layerDims.append(layerDim.copy())\n",
    "                    weightScale = layer['wScale']   if 'wScale'   in layer.keys() else 100\n",
    "                    delay       = layer['delay']    if 'delay'    in layer.keys() else False\n",
    "                    maxDelay    = layer['maxDelay'] if 'maxDelay' in layer.keys() else 62\n",
    "                    \n",
    "                    blocks.append(recurrentBlock(slayer, layerDim[0], params[0], weightScale, self.preHookFx, \n",
    "                                  self.weightNorm, delay, maxDelay, self.countLog))\n",
    "                    layerDim[0] = params[0]\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    print(self._tableStr('Recurrent', layerDim[2], layerDim[1], layerDim[0], delay=delay, \n",
    "                                        numParams=sum(p.numel() for p in blocks[-1].parameters() if p.requires_grad)))\n",
    "                elif layerType == 'average':\n",
    "                    params = [int(i) for i in re.findall(r'\\d+', layer['dim'])]\n",
    "                    layerDim[0] = params[0]\n",
    "                    layerDim[1] = layerDim[2] = 1\n",
    "                    self.layerDims.append(layerDim.copy())\n",
    "\n",
    "                    blocks.append(averageBlock(nOutputs=layerDim[0], countLog=self.countLog))\n",
    "                    print(self._tableStr('Average', 1, 1, params[0]))\n",
    "\n",
    "                i += 1\n",
    "        self.nOutput = layerDim[0] * layerDim[1] * layerDim[2]\n",
    "        print(self._tableStr(numParams=sum(p.numel() for p in blocks.parameters() if p.requires_grad), footer=True))\n",
    "        return blocks\n",
    "    \n",
    "    \n",
    "    def genSpyModel(self, fname):\n",
    "        qWeights = lambda x: self.preHookFx(x).cpu().data.numpy().squeeze()\n",
    "\n",
    "        h = h5py.File(fname, 'w')\n",
    "\n",
    "        simulation = h.create_group('simulation')\n",
    "\n",
    "        for key, value in self.netParams['simulation'].items():\n",
    "            # print(key, value)\n",
    "            simulation[key] = value\n",
    "\n",
    "        layer = h.create_group('layer')\n",
    "        layer.create_dataset('0/type', (1, ), 'S10', [b'input'])\n",
    "        layer.create_dataset('0/shape', data=np.array([self.inputShape[2], self.inputShape[1], self.inputShape[0]]))\n",
    "        \n",
    "        for i, block in enumerate(self.blocks):\n",
    "            print(\"\\nblock %d / %d\" % (i, len(self.blocks)))\n",
    "            layerType = block.__class__.__name__[:-5]\n",
    "            \n",
    "            print(layerType.encode('ascii', 'ignore'))\n",
    "            layer.create_dataset('{}/type'.format(i+1), (1, ), 'S10', [layerType.encode('ascii', 'ignore')])\n",
    "            \n",
    "            print(i, self.layerDims[i])\n",
    "            layer.create_dataset('{}/shape'.format(i+1), data=np.array(self.layerDims[i]))\n",
    "            \n",
    "            if layerType != 'flatten':\n",
    "                layer.create_dataset('{}/weight'.format(i+1), data=qWeights(torch.Tensor(self.weights[i])))\n",
    "                if layerType == 'recurrent':\n",
    "                    layer.create_dataset('{}/recWeight'.format(i+1), data=qWeights(torch.Tensor(self.recWeights)))\n",
    "                    \n",
    "            for key, param in block.paramsDict.items():\n",
    "                layer.create_dataset('{}/{}'.format(i+1, key), data=param)\n",
    "                \n",
    "            if layerType != 'flatten' and layerType != 'average':\n",
    "                for key, value in block.slayer.neuron.items():\n",
    "                    # print(i, key, value)\n",
    "                    layer.create_dataset('{}/neuron/{}'.format(i+1, key), data=value)\n",
    "        h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation:\n",
      "    Ts         : 1\n",
      "    tSample    : 270\n",
      "\n",
      "neuron:\n",
      "    type       : LOIHI\n",
      "    vThMant    : 99\n",
      "    vDecay     : 282\n",
      "    iDecay     : 2090\n",
      "    refDelay   : 1\n",
      "    wgtExp     : 0\n",
      "    tauRho     : 1\n",
      "    scaleRho   : 1\n",
      "\n",
      "Max PSP kernel: 99.0\n",
      "Scaling neuron[scaleRho] by Max PSP Kernel @slayerLoihi\n",
      "\n",
      "Network Architecture:\n",
      "|   Type   |  W  |  H  |  C  | ker | str | pad |delay|  params  |\n",
      "|Input     |   48|    1|    1|     |     |     |False|          |\n",
      "|Dense     |    1|    1|  450|     |     |     |False|     21600|\n",
      "|Dense     |    1|    1|   28|     |     |     |False|     12600|\n",
      "|Total                                               |     34200|\n",
      "TODO core usage estimator\n"
     ]
    }
   ],
   "source": [
    "# Create the network\n",
    "netLoihi = SpyTorch2Loihi(netParams, spy_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "block 0 / 3\n",
      "b'flatten'\n",
      "0 [48, 1, 1]\n",
      "\n",
      "block 1 / 3\n",
      "b'dense'\n",
      "1 [450, 1, 1]\n",
      "\n",
      "block 2 / 3\n",
      "b'dense'\n",
      "2 [28, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Export the model\n",
    "netLoihi.genSpyModel('../netsLoihi/netLoihi_fwd_th' + str(threshold) + run + '.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9bf43d2b4a4b64acce80ec436e45972ca3a2814376cdde651dbbddec46144e4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('pyenv_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
